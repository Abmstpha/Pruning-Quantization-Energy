# -*- coding: utf-8 -*-
"""TP3_greenAI_workshop.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Rm8cyoR9H8sYnMmCILYS5DxQG-niHVYt

## Pruning & Quantization under Energy Constraints

# 0) Install deps----
"""

!pip -q install torch==2.8.0 torchvision timm pandas==2.2.2 openpyxl ptflops thop torch-pruning --upgrade

!pip install -U codecarbon==2.8.4

"""# 1) Drive mount & outputs"""

# ==========
from google.colab import drive
from pathlib import Path
drive.mount('/content/drive')

RESULTS_DIR = Path("/content/drive/MyDrive/codecarbon_tp3")
RESULTS_DIR.mkdir(parents=True, exist_ok=True)

# Per-scenario outputs
CSV_BASELINE  = RESULTS_DIR / "df_baseline.csv"
XLSX_BASELINE = RESULTS_DIR / "df_baseline.xlsx"

CSV_UNSTR_PTQ  = RESULTS_DIR / "df_unstructured_ptq.csv"
XLSX_UNSTR_PTQ = RESULTS_DIR / "df_unstructured_ptq.xlsx"

CSV_STRUCT     = RESULTS_DIR / "df_structured.csv"
XLSX_STRUCT    = RESULTS_DIR / "df_structured.xlsx"

CSV_QAT        = RESULTS_DIR / "df_qat.csv"
XLSX_QAT       = RESULTS_DIR / "df_qat.xlsx"

CSV_FULL       = RESULTS_DIR / "df_full.csv"
XLSX_FULL      = RESULTS_DIR / "df_full.xlsx"

CSV_FINAL      = RESULTS_DIR / "results_tp3.csv"
XLSX_FINAL     = RESULTS_DIR / "results_tp3.xlsx"

EMISSIONS_CSV  = RESULTS_DIR / "emissions.csv"

print("All outputs ->", RESULTS_DIR)

"""# 2) Imports & globals"""

# ==========
import os, time, math, io, random, json
from typing import Optional, Tuple, Dict, List
from pathlib import Path
import time, uuid, wandb
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, Subset
from torchvision import datasets, transforms

import pandas as pd
from codecarbon import EmissionsTracker

import timm
import thop
from ptflops import get_model_complexity_info
import torch_pruning as tp
import copy

# --- Repro & perf knobs ---
SEED = 1337
random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)
torch.backends.cudnn.benchmark = True
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
try: torch.set_float32_matmul_precision("high")
except Exception: pass

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
DEFAULT_IMG = 224
DATA_ROOT = "data"

print("DEVICE:", DEVICE)



PTQ_DEVICE = torch.device("cpu")
torch.set_num_threads(1)

print("PTQ_DEVICE:", PTQ_DEVICE)

"""## 2.1) W&B setup"""

# ==========
import wandb, os, time, uuid
try:
    from google.colab import userdata
except ImportError:
    userdata = None


def _get_secret(name: str, default: str = None, required: bool = True) -> str:
    val = None
    if userdata is not None:
        try:
            val = userdata.get(name)
        except Exception:
            val = None
    if not val:
        val = os.environ.get(name, default)
    if required and (val is None or str(val).strip() == ""):
        raise RuntimeError(f"Missing secret: {name}. Add it in Colab Secrets or set env var.")
    return val


WANDB_API_KEY = _get_secret("WANDB_API_KEY2", required=True)
WB_ENTITY  = _get_secret("WANDB_ENTITY",  default="esp1", required=False)
WB_PROJECT = _get_secret("WANDB_PROJECT", default="greenAi_tp3", required=False)
WB_TAGS    = _get_secret("WANDB_TAGS",    default="tp3,codecarbon,compression", required=False).split(",")

os.environ["WANDB_API_KEY"] = WANDB_API_KEY
wandb.login(key=WANDB_API_KEY, relogin=True)
print(f"W&B ready — entity={WB_ENTITY}, project={WB_PROJECT}, tags={WB_TAGS}")


def wandb_start(run_name: str, config: dict):
    uniq = time.strftime("%H%M%S") + "-" + uuid.uuid4().hex[:4]
    return wandb.init(
        entity=WB_ENTITY,
        project=WB_PROJECT,
        name=f"{run_name}-{uniq}",
        config=config,
        reinit=True,
        resume="never",
        tags=WB_TAGS,
    )

"""# 3) CodeCarbon helpers"""

def _cc_rows_count(csv_path: Path) -> int:
    if not csv_path.exists(): return 0
    try: return len(pd.read_csv(csv_path))
    except Exception: return 0

def _cc_last_added(csv_path: Path, start_index: int):
    if not csv_path.exists(): return (None, None, {})
    df = pd.read_csv(csv_path)
    if df.empty: return (None, None, {})
    df2 = df.iloc[start_index:] if len(df) > start_index else df
    row = df2.iloc[-1].to_dict()
    ekwh = float(row.get("energy_consumed")) if pd.notna(row.get("energy_consumed")) else None
    ekg  = float(row.get("emissions"))       if pd.notna(row.get("emissions"))       else None
    meta = {
        "country_name": row.get("country_name"),
        "country_iso":  row.get("country_iso"),
        "region":       row.get("region"),
        "cloud":        row.get("cloud_provider"),
        "cloud_region": row.get("cloud_region"),
    }
    return ekwh, ekg, meta

# --- Robust cc_section: handles both country_iso and country_iso_code ---
from contextlib import contextmanager
import inspect
from codecarbon import EmissionsTracker

@contextmanager
def cc_section(section_name: str, output_dir: Path = RESULTS_DIR, country_iso: str = os.getenv("CC_ISO","FRA")):
    base_kwargs = dict(
        project_name=f"tp3::{section_name}",
        output_dir=str(output_dir),
        save_to_file=True,
        measure_power_secs=1,
        log_level="warning",
    )
    sig = inspect.signature(EmissionsTracker.__init__)
    if "country_iso" in sig.parameters:
        base_kwargs["country_iso"] = country_iso
    elif "country_iso_code" in sig.parameters:
        base_kwargs["country_iso_code"] = country_iso
    # else: omit — CodeCarbon will use its default

    tracker = EmissionsTracker(**base_kwargs)
    before = _cc_rows_count(EMISSIONS_CSV)
    tracker.start()
    try:
        yield
    finally:
        _ = tracker.stop()
        ekwh, ekg, meta = _cc_last_added(EMISSIONS_CSV, before)
        cc_section.last = {"energy_kwh": ekwh, "emissions_kg": ekg, **meta}

"""# 4) Dataset & loaders (Flowers102, same TP2 transforms)"""

def _transforms(img_size=DEFAULT_IMG):
    common = [
        transforms.Resize((img_size, img_size)),
        transforms.ToTensor(),
        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),
    ]
    return transforms.Compose(common), transforms.Compose(common)

def load_flowers102(limit: Optional[int]=None, img_size=DEFAULT_IMG):
    tr_t, te_t = _transforms(img_size)
    ds_tr = datasets.Flowers102(root=DATA_ROOT, split="train", download=True, transform=tr_t)
    ds_te = datasets.Flowers102(root=DATA_ROOT, split="test",  download=True, transform=te_t)
    if limit is not None:
        ds_tr = Subset(ds_tr, list(range(min(len(ds_tr), limit))))
        ds_te = Subset(ds_te, list(range(min(len(ds_te), max(1, limit//5)))))
    return ds_tr, ds_te, 102

def make_loaders(ds_tr, ds_te, batch_train=32, batch_eval=64):
    import multiprocessing as mp
    ncpu = max(2, (mp.cpu_count() or 4)//2)
    tr = DataLoader(ds_tr, batch_size=batch_train, shuffle=True,
                    num_workers=ncpu, pin_memory=(DEVICE.type=="cuda"),
                    prefetch_factor=2, persistent_workers=True)
    te = DataLoader(ds_te, batch_size=batch_eval, shuffle=False,
                    num_workers=ncpu, pin_memory=(DEVICE.type=="cuda"),
                    prefetch_factor=2, persistent_workers=True)
    return tr, te

"""# 5) Backbones (ResNet18, MLP-Mixer B16)"""

def make_resnet18(num_classes: int, pretrained=True) -> nn.Module:
    from torchvision.models import resnet18, ResNet18_Weights
    m = resnet18(weights=(ResNet18_Weights.DEFAULT if pretrained else None))
    m.fc = nn.Linear(m.fc.in_features, num_classes)
    return m

def make_mlp_mixer_b16(num_classes: int, pretrained=True) -> nn.Module:
    return timm.create_model("mixer_b16_224", pretrained=pretrained, num_classes=num_classes)

def freeze_all_but_head(model: nn.Module):
    for p in model.parameters(): p.requires_grad = False
    for attr in ["fc","classifier","head"]:
        if hasattr(model, attr):
            for p in getattr(model, attr).parameters(): p.requires_grad = True
            return
    last = None
    for m in model.modules():
        if isinstance(m, nn.Linear): last = m
    if last is not None:
        for p in last.parameters(): p.requires_grad = True

"""# 6) Training / eval utils"""

from contextlib import nullcontext
AMP = (DEVICE.type == "cuda")
amp_ctx = torch.cuda.amp.autocast if AMP else nullcontext
scaler = torch.cuda.amp.GradScaler(enabled=AMP)

# CUDA/AMP path for normal (non-quantized) models
@torch.no_grad()
def evaluate(model, loader):
    # keep model + inputs on DEVICE (GPU if available)
    model = model.to(DEVICE)
    model.eval()
    correct = 0; total = 0
    t0 = time.time()
    if DEVICE.type == "cuda":
        torch.cuda.reset_peak_memory_stats()
    with torch.inference_mode(), amp_ctx():
        for x, y in loader:
            x = x.to(DEVICE, non_blocking=True); y = y.to(DEVICE, non_blocking=True)
            logits = model(x)
            pred = logits.argmax(1)
            correct += (pred == y).sum().item()
            total   += y.numel()
    latency_ms = (time.time() - t0)/max(1,total)*1000.0
    peak_mb = torch.cuda.max_memory_allocated()/(1024**2) if DEVICE.type=="cuda" else None
    return correct/max(1,total), latency_ms, peak_mb

# CPU-only path for quantized models (PTQ / QAT exported to int8)
@torch.no_grad()
def evaluate_on(model: nn.Module, loader, device: torch.device):
    model = model.to(device).eval()
    correct = 0; total = 0
    t0 = time.time()
    with torch.inference_mode():
        for x, y in loader:
            x = x.to(device, non_blocking=(device.type=="cuda"))
            y = y.to(device, non_blocking=(device.type=="cuda"))
            logits = model(x)
            pred = logits.argmax(1)
            correct += (pred == y).sum().item()
            total   += y.numel()
    latency_ms = (time.time() - t0)/max(1,total)*1000.0
    peak_mb = torch.cuda.max_memory_allocated()/(1024**2) if device.type=="cuda" else None
    return correct/max(1,total), latency_ms, peak_mb

@torch.no_grad()
def qat_warmup_observers(model: nn.Module, loader, num_batches: int = 10, device: torch.device = None):
    """Run a few batches in train mode so FakeQuant observers get sane min/max."""
    device = device or (next(model.parameters()).device)
    was_training = model.training
    model.train()
    seen = 0
    for x, _ in loader:
        x = x.to(device, non_blocking=(device.type == "cuda"))
        model(x)
        seen += 1
        if seen >= num_batches:
            break
    model.train(was_training)

def train_epoch(model, loader, optim_, criterion):
    model.train()
    running = 0.0
    for x,y in loader:
        x = x.to(DEVICE, non_blocking=True); y = y.to(DEVICE, non_blocking=True)
        with amp_ctx():
            out = model(x)
            loss = criterion(out, y)
        optim_.zero_grad(set_to_none=True)
        scaler.scale(loss).backward()
        scaler.step(optim_); scaler.update()
        running += loss.item()
    return running / max(1,len(loader))

"""# 7) Model accounting: Params, FLOPs, Size"""

# ==========
def count_params_m(model: nn.Module) -> float:
    return sum(p.numel() for p in model.parameters())/1e6

def estimate_flops_g(model: nn.Module, img_size: int = 224) -> float:
    # Make a CPU copy in eval and disable fake-quant/observers
    model_cpu = copy.deepcopy(model).to("cpu").eval()
    try:
        # Try the official utilities first
        import torch.ao.quantization as aoq
        try: aoq.disable_fake_quant(model_cpu)
        except Exception: pass
        try: aoq.disable_observer(model_cpu)
        except Exception: pass
    except Exception:
        # Fallback: best-effort manual disable on modules that expose the hooks
        for m in model_cpu.modules():
            if hasattr(m, "disable_fake_quant"): m.disable_fake_quant()
            if hasattr(m, "disable_observer"):  m.disable_observer()

    try:
        macs, _ = get_model_complexity_info(
            model_cpu, (3, img_size, img_size), as_strings=False,
            print_per_layer_stat=False, verbose=False
        )
        return float(macs * 2) / 1e9
    except Exception:
        dummy = torch.randn(1, 3, img_size, img_size)
        flops, _ = thop.profile(model_cpu, inputs=(dummy,), verbose=False)
        return float(flops) / 1e9

def model_size_mb(model: nn.Module) -> float:
    buf = io.BytesIO()
    torch.save(model.state_dict(), buf)
    return len(buf.getvalue()) / (1024**2)

"""# 8) Pruning utilities

"""

import torch.nn.utils.prune as prune

def global_unstructured_prune(model: nn.Module, amount: float):
    parameters_to_prune = []
    for module in model.modules():
        if isinstance(module, (nn.Conv2d, nn.Linear)):
            parameters_to_prune.append((module, 'weight'))
    if not parameters_to_prune: return
    prune.global_unstructured(
        parameters_to_prune,
        pruning_method=prune.L1Unstructured,
        amount=amount
    )
    for (m, _) in parameters_to_prune:
        try: prune.remove(m, 'weight')
        except Exception: pass

# --- Structured/channel pruning (dependency-aware, BN-safe) ---

def structured_channel_prune_resnet(
    model: nn.Module,
    flops_reduction_target: float = 0.30,
    img_size: int = 224
):
    """
    ResNet-style pruning: globally prune conv output channels to hit ~flops_reduction_target.
    Keeps the stem conv and final classifier intact.
    """
    model = model.to(DEVICE).eval()
    example_inputs = torch.randn(1, 3, img_size, img_size).to(DEVICE)

    # Ignore stem conv and final classifier head
    ignored_layers = set()
    for name, m in model.named_modules():
        if isinstance(m, nn.Conv2d) and name.startswith("conv1"):
            ignored_layers.add(m)
    if hasattr(model, "fc"):
        ignored_layers.add(model.fc)

    # L1 importance, global channel pruning
    imp = tp.importance.MagnitudeImportance(p=1)
    pruner = tp.pruner.MagnitudePruner(
        model,
        example_inputs=example_inputs,
        importance=imp,
        global_pruning=True,
        pruning_ratio=flops_reduction_target,   # e.g., 0.30 => ~30% FLOPs cut
        iterative_steps=1,
        ignored_layers=list(ignored_layers),
    )
    pruner.step()
    return model


def structured_channel_prune_mixer(
    model: nn.Module,
    flops_reduction_target: float = 0.30,
    img_size: int = 224
):
    """
    MLP-Mixer pruning: globally prune large Linear outputs while keeping the final classifier intact.
    """
    model = model.to(DEVICE).eval()
    example_inputs = torch.randn(1, 3, img_size, img_size).to(DEVICE)

    # Ignore the final head/classifier layer
    ignored_layers = set()
    for attr in ["head", "classifier", "fc"]:
        if hasattr(model, attr):
            ignored_layers.add(getattr(model, attr))

    imp = tp.importance.MagnitudeImportance(p=1)
    pruner = tp.pruner.MagnitudePruner(
        model,
        example_inputs=example_inputs,
        importance=imp,
        global_pruning=True,
        pruning_ratio=flops_reduction_target,
        iterative_steps=1,
        ignored_layers=list(ignored_layers),
    )
    pruner.step()
    return model

"""# 9) Quantization utilities"""

def apply_ptq_dynamic(model: nn.Module) -> nn.Module:
    # Work on a copy so the original stays on CUDA
    model_copy = copy.deepcopy(model).to("cpu").eval()
    qmodel = torch.quantization.quantize_dynamic(
        model_copy, {nn.Linear}, dtype=torch.qint8
    )
    return qmodel

from torch.ao.quantization.fake_quantize import FakeQuantize
from torch.ao.quantization.observer import MovingAverageMinMaxObserver

class QATLinear(nn.Module):
    def __init__(self, linear: nn.Linear):
        super().__init__()
        self.linear  = linear
        self.act_fake = FakeQuantize(
            observer=MovingAverageMinMaxObserver,
            dtype=torch.quint8,
            qscheme=torch.per_tensor_affine,
        )
        self.w_fake   = FakeQuantize(
            observer=MovingAverageMinMaxObserver,
            dtype=torch.qint8,
            qscheme=torch.per_tensor_symmetric,
        )

    def forward(self, x):
        # keep fake-quant modules on the same device as inputs
        self.act_fake.to(x.device)
        self.w_fake.to(x.device)

        xq = self.act_fake(x)
        Wq = self.w_fake(self.linear.weight)
        b  = self.linear.bias                      # ← your bias stays here
        return F.linear(xq, Wq, b)

def replace_head_with_qat(model: nn.Module):
    device = next(model.parameters()).device  # where the model currently lives

    for attr in ["fc","classifier","head"]:
        if hasattr(model, attr):
            head = getattr(model, attr)
            if isinstance(head, nn.Linear):
                setattr(model, attr, QATLinear(head).to(device))
            elif isinstance(head, nn.Sequential):
                mods = list(head.children())
                if isinstance(mods[-1], nn.Linear):
                    mods[-1] = QATLinear(mods[-1]).to(device)
                    setattr(model, attr, nn.Sequential(*mods).to(device))
            return model
    return model

# --- Helper: BatchNorm recalibration ---
@torch.no_grad()
def bn_recalibrate(model: nn.Module, loader, num_batches: int = 10, device: torch.device = DEVICE):
    """Refresh BN running_mean / running_var after structured pruning."""
    was_training = model.training
    model.train()
    seen = 0
    for x, _ in loader:
        x = x.to(device, non_blocking=(device.type == "cuda"))
        model(x)
        seen += 1
        if seen >= num_batches:
            break
    model.train(was_training)

"""# 10) Scenario runners"""

COLS = ["Backbone","Scenario","Dataset","Top-1 Acc.","Model Size (MB)","Params (M)","FLOPs (G)",
        "Energy (kWh)","Emissions (kgCO2eq)","Train Time (s)","Latency (ms/img)","Peak Mem (MB)",
        "Eval Device"]

def _append_row(df: pd.DataFrame, row: dict) -> pd.DataFrame:
    cols = list(df.columns)
    return pd.concat([df, pd.DataFrame([[row.get(c,None) for c in cols]], columns=cols)], ignore_index=True)

def _save_df(df: pd.DataFrame, csv_path: Path, xlsx_path: Path):
    df.to_csv(csv_path, index=False)
    try: df.to_excel(xlsx_path, index=False)
    except Exception: pass

# common train/eval + metrics helper
def run_train_eval(
    model: nn.Module,
    tr_loader, te_loader,
    epochs: int,
    lr: float = 1e-3,
    label_smoothing: float = 0.0,
    measure_section_name: str = "train",
    wandb_run: "wandb.sdk.wandb_run.Run" = None
) -> Tuple[float, float, float, float, float, float]:
    crit = nn.CrossEntropyLoss(label_smoothing=label_smoothing)
    opt  = optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=lr, weight_decay=0.01,
                       fused=(DEVICE.type=="cuda"))

    t0 = time.time()
    with cc_section(measure_section_name):
        for ep in range(1, epochs+1):
            loss_ep = train_epoch(model, tr_loader, opt, crit)
            if wandb_run is not None:
                wandb_run.log({"epoch": ep, "train_loss": loss_ep})
    train_time = time.time() - t0
    e_kwh = cc_section.last.get("energy_kwh")
    e_kg  = cc_section.last.get("emissions_kg")

    acc, latency_ms, peak_mem = evaluate(model, te_loader)
    if wandb_run is not None:
        wandb_run.log({"val_accuracy": acc, "latency_ms_per_image": latency_ms, "peak_mem_mb": peak_mem,
                       "train_time_s": train_time, "energy_kwh": e_kwh, "emissions_kg": e_kg})
    return acc, train_time, e_kwh, e_kg, latency_ms, peak_mem

"""## ---- Scenario 1: Baseline FT (reference) ----"""

def scenario_baseline(backbone_name: str, ds_tr, ds_te, epochs:int=10, batch_train=32, batch_eval=64):
    df = pd.DataFrame(columns=COLS)
    tr_loader, te_loader = make_loaders(ds_tr, ds_te, batch_train, batch_eval)
    NUM_C = 102

    if backbone_name == "resnet18":
        model = make_resnet18(NUM_C, pretrained=True).to(DEVICE)
        freeze_all_but_head(model)
    elif backbone_name == "mixer_b16_224":
        model = make_mlp_mixer_b16(NUM_C, pretrained=True).to(DEVICE)
        freeze_all_but_head(model)
    else:
        raise ValueError("Unknown backbone")

    cfg = {
        "scenario": "Baseline FT", "backbone": backbone_name, "dataset": "Flowers102",
        "epochs": epochs, "batch_train": batch_train, "batch_eval": batch_eval,
        "limit_trainset": LIMIT_TRAINSET
    }
    wb = wandb_start(run_name=f"{backbone_name}-baseline-ft", config=cfg)

    acc, train_time, e_kwh, e_kg, lat_ms, peak = run_train_eval(
        model, tr_loader, te_loader, epochs=epochs, lr=1e-3,
        label_smoothing=0.1, measure_section_name=f"baseline::{backbone_name}", wandb_run=wb
    )

    size_mb = model_size_mb(model)
    params_m = count_params_m(model)
    flops_g  = estimate_flops_g(model)

    wb.summary.update({
        "final_val_accuracy": acc, "energy_kwh": e_kwh, "emissions_kg": e_kg,
        "train_time_s": train_time, "latency_ms_per_image": lat_ms, "peak_mem_mb": peak,
        "model_size_mb": size_mb, "params_m": params_m, "flops_g": flops_g
    })
    wb.finish()

    row = {
        "Backbone": backbone_name, "Scenario": "Baseline FT", "Dataset": "Flowers102",
        "Top-1 Acc.": acc, "Model Size (MB)": size_mb, "Params (M)": params_m, "FLOPs (G)": flops_g,
        "Energy (kWh)": e_kwh, "Emissions (kgCO2eq)": e_kg, "Train Time (s)": train_time,
        "Latency (ms/img)": lat_ms, "Peak Mem (MB)": peak,
        "Eval Device": DEVICE.type,    # <— added
    }
    df = _append_row(df, row)
    return df

"""## ---- Scenario 2: Unstructured pruning + light FT + PTQ ----"""

## ---- Scenario 2: Unstructured pruning + light FT + PTQ (FINAL) ----
def scenario_unstructured_ptq(
    backbone_name: str,
    ds_tr, ds_te,
    sparsities=(0.3, 0.6),
    ft_epochs=3,
    batch_train=32, batch_eval=64
):
    df = pd.DataFrame(columns=COLS)
    NUM_C = 102
    dataset_name = "Flowers102"

    for s in sparsities:
        tr_loader, te_loader = make_loaders(ds_tr, ds_te, batch_train, batch_eval)

        # ----- Build backbone (pretrained) -----
        if backbone_name == "resnet18":
            model = make_resnet18(NUM_C, pretrained=True).to(DEVICE)
            freeze_all_but_head(model)
        elif backbone_name == "mixer_b16_224":
            model = make_mlp_mixer_b16(NUM_C, pretrained=True).to(DEVICE)
            freeze_all_but_head(model)
        else:
            raise ValueError(f"Unknown backbone: {backbone_name}")

        # ----- Global magnitude pruning (unstructured) -----
        global_unstructured_prune(model, amount=s)
        model = model.to(DEVICE)  # ensure on GPU if available

        # ----- W&B run -----
        cfg = {
            "scenario": "Unstructured+PTQ",
            "backbone": backbone_name,
            "dataset": dataset_name,
            "sparsity": s,
            "ft_epochs": ft_epochs,
            "limit_trainset": LIMIT_TRAINSET,
            "batch_train": batch_train,
            "batch_eval": batch_eval,
        }
        wb = wandb_start(run_name=f"{backbone_name}-unstructured-ptq-s{int(s*100)}", config=cfg)

        # ----- Light FT after pruning -----
        acc_ft, train_time, e_kwh_ft, e_kg_ft, lat_ms_ft, peak_ft = run_train_eval(
            model, tr_loader, te_loader,
            epochs=ft_epochs, lr=1e-3, label_smoothing=0.1,
            measure_section_name=f"unstructured::{backbone_name}::s{int(s*100)}",
            wandb_run=wb
        )

        # ----- PTQ (dynamic int8) — evaluate on CPU only -----
        qmodel = apply_ptq_dynamic(model)  # deepcopy -> CPU int8 where applicable

        # Measure quantized model size (MB)
        q_buf = io.BytesIO()
        try:
            torch.save(qmodel.state_dict(), q_buf)
            quant_size_mb = len(q_buf.getvalue()) / (1024**2)
        except Exception:
            quant_size_mb = None

        with cc_section(f"ptq_infer::{backbone_name}::s{int(s*100)}"):
            q_acc, q_lat_ms, q_peak = evaluate_on(qmodel, te_loader, PTQ_DEVICE)
        e_kwh_inf = cc_section.last.get("energy_kwh")
        e_kg_inf  = cc_section.last.get("emissions_kg")

        # ----- Model accounting from pruned (float) model -----
        size_mb_float = model_size_mb(model)    # size of the pruned float model
        params_m = count_params_m(model)
        flops_g  = estimate_flops_g(model)

        # ----- W&B summary -----
        wb.summary.update({
            # FT metrics
            "ft_val_accuracy": acc_ft,
            "ft_train_time_s": train_time,
            "ft_energy_kwh": e_kwh_ft,
            "ft_emissions_kg": e_kg_ft,
            "ft_latency_ms_per_image": lat_ms_ft,
            "ft_peak_mem_mb": peak_ft,
            # PTQ metrics
            "ptq_val_accuracy": q_acc,
            "ptq_latency_ms_per_image": q_lat_ms,
            "ptq_energy_kwh": e_kwh_inf,
            "ptq_emissions_kg": e_kg_inf,
            # model stats
            "float_model_size_mb": size_mb_float,
            "quant_model_size_mb": quant_size_mb,
            "params_m": params_m,
            "flops_g": flops_g,
        })
        wb.finish()

        # ----- Row: report PTQ inference results, store QUANTIZED size when available -----
        row = {
            "Backbone": backbone_name,
            "Scenario": f"Unstructured+PTQ (s={int(s*100)}%)",
            "Dataset": dataset_name,
            "Top-1 Acc.": q_acc,
            "Model Size (MB)": (quant_size_mb if quant_size_mb is not None else size_mb_float),
            "Params (M)": params_m,
            "FLOPs (G)": flops_g,
            "Energy (kWh)": e_kwh_inf,
            "Emissions (kgCO2eq)": e_kg_inf,
            "Train Time (s)": train_time,
            "Latency (ms/img)": q_lat_ms,
            "Peak Mem (MB)": q_peak,
            "Eval Device": PTQ_DEVICE.type,
        }
        df = _append_row(df, row)

    return df



"""## ---- Scenario 3: Structured/channel pruning (~30% FLOPs) + light FT ----"""

def scenario_structured(
    backbone_name: str,
    ds_tr, ds_te,
    flops_drop=0.30,
    ft_epochs=5,
    batch_train=32, batch_eval=64
):
    df = pd.DataFrame(columns=COLS)
    NUM_C = 102
    dataset_name = "Flowers102"

    tr_loader, te_loader = make_loaders(ds_tr, ds_te, batch_train, batch_eval)

    # ----- Build + prune structurally -----
    if backbone_name == "resnet18":
        model = make_resnet18(NUM_C, pretrained=True).to(DEVICE)
        freeze_all_but_head(model)
        # (optional) FLOPs before pruning for reporting
        flops_before = estimate_flops_g(model, DEFAULT_IMG)
        model = structured_channel_prune_resnet(model, flops_reduction_target=flops_drop, img_size=DEFAULT_IMG)
    elif backbone_name == "mixer_b16_224":
        model = make_mlp_mixer_b16(NUM_C, pretrained=True).to(DEVICE)
        freeze_all_but_head(model)
        flops_before = estimate_flops_g(model, DEFAULT_IMG)
        model = structured_channel_prune_mixer(model, flops_reduction_target=flops_drop, img_size=DEFAULT_IMG)
    else:
        raise ValueError(f"Unknown backbone: {backbone_name}")

    model = model.to(DEVICE)
    # --- Recalibrate BN stats after structural pruning ---
    warm_loader, _ = make_loaders(ds_tr, ds_te, batch_train=min(16, batch_train), batch_eval=batch_eval)
    bn_recalibrate(model, warm_loader, num_batches=10, device=DEVICE)

    # ----- W&B run -----
    cfg = {
        "scenario": f"Structured (~{int(flops_drop*100)}% FLOPs)",
        "backbone": backbone_name,
        "dataset": dataset_name,
        "flops_drop_target": flops_drop,   # <-- log target
        "ft_epochs": ft_epochs,
        "limit_trainset": LIMIT_TRAINSET,
        "batch_train": batch_train,
        "batch_eval": batch_eval,
    }
    wb = wandb_start(run_name=f"{backbone_name}-structured-{int(flops_drop*100)}", config=cfg)

    # ----- Light FT after structured pruning -----
    acc, train_time, e_kwh, e_kg, lat_ms, peak = run_train_eval(
        model, tr_loader, te_loader,
        epochs=ft_epochs, lr=1e-3, label_smoothing=0.1,
        measure_section_name=f"structured::{backbone_name}::drop{int(flops_drop*100)}",
        wandb_run=wb
    )

    # ----- Model accounting -----
    size_mb  = model_size_mb(model)
    params_m = count_params_m(model)
    flops_after  = estimate_flops_g(model)         # <-- after-prune FLOPs
    flops_drop_pct = 100.0 * (1.0 - flops_after / max(flops_before, 1e-9))  # <-- achieved %

    # ----- W&B summary -----
    wb.summary.update({
        "final_val_accuracy": acc,
        "train_time_s": train_time,
        "energy_kwh": e_kwh,
        "emissions_kg": e_kg,
        "latency_ms_per_image": lat_ms,
        "peak_mem_mb": peak,
        "model_size_mb": size_mb,
        "params_m": params_m,
        "flops_g_before": flops_before,
        "flops_g_after": flops_after,
        "flops_drop_percent": flops_drop_pct,
    })
    wb.finish()

    # ----- Row -----
    row = {
        "Backbone": backbone_name,
        "Scenario": f"Structured (~{int(flops_drop*100)}% FLOPs)",
        "Dataset": dataset_name,
        "Top-1 Acc.": acc,
        "Model Size (MB)": size_mb,
        "Params (M)": params_m,
        "FLOPs (G)": flops_after,          # store the *actual* FLOPs
        "Energy (kWh)": e_kwh,
        "Emissions (kgCO2eq)": e_kg,
        "Train Time (s)": train_time,
        "Latency (ms/img)": lat_ms,
        "Peak Mem (MB)": peak,
        "Eval Device": DEVICE.type,
        # optional extra column if you added it to COLS:
        # "FLOPs Drop (%)": flops_drop_pct,
    }
    df = _append_row(df, row)
    return df

"""## ---- Scenario 4: QAT (head-only) ----"""

def scenario_qat(backbone_name: str, ds_tr, ds_te, epochs:int=10, batch_train=32, batch_eval=64):
    df = pd.DataFrame(columns=COLS)
    tr_loader, te_loader = make_loaders(ds_tr, ds_te, batch_train, batch_eval)
    NUM_C = 102

    # ----- Build backbone -----
    if backbone_name == "resnet18":
        model = make_resnet18(NUM_C, pretrained=True).to(DEVICE)
        freeze_all_but_head(model)
    elif backbone_name == "mixer_b16_224":
        model = make_mlp_mixer_b16(NUM_C, pretrained=True).to(DEVICE)
        freeze_all_but_head(model)
    else:
        raise ValueError("Unknown backbone")

    # ----- QAT head + observer warmup -----
    model = replace_head_with_qat(model).to(DEVICE)
    warm_loader, _ = make_loaders(ds_tr, ds_te, batch_train=min(16, batch_train), batch_eval=batch_eval)
    qat_warmup_observers(model, warm_loader, num_batches=10, device=DEVICE)

    # ----- W&B run -----
    cfg = {
        "scenario": "QAT (head)",
        "backbone": backbone_name,
        "dataset": "Flowers102",
        "epochs": epochs, "batch_train": batch_train, "batch_eval": batch_eval,
        "limit_trainset": LIMIT_TRAINSET
    }
    wb = wandb_start(run_name=f"{backbone_name}-qat-head", config=cfg)

    # ----- Train/eval -----
    acc, train_time, e_kwh, e_kg, lat_ms, peak = run_train_eval(
        model, tr_loader, te_loader, epochs=epochs, lr=1e-3,
        label_smoothing=0.1, measure_section_name=f"qat::{backbone_name}", wandb_run=wb
    )

    # ----- Accounting (FLOPs can fail with fake-quant → guard) -----
    size_mb = model_size_mb(model)
    params_m = count_params_m(model)
    try:
        flops_g = estimate_flops_g(model)
    except Exception:
        flops_g = None

    wb.summary.update({
        "final_val_accuracy": acc, "energy_kwh": e_kwh, "emissions_kg": e_kg,
        "train_time_s": train_time, "latency_ms_per_image": lat_ms, "peak_mem_mb": peak,
        "model_size_mb": size_mb, "params_m": params_m, "flops_g": flops_g
    })
    wb.finish()

    # ----- Row (Option A parity: include eval device) -----
    row = {
        "Backbone": backbone_name,
        "Scenario": "QAT (head)",
        "Dataset": "Flowers102",
        "Top-1 Acc.": acc,
        "Model Size (MB)": size_mb,
        "Params (M)": params_m,
        "FLOPs (G)": flops_g,
        "Energy (kWh)": e_kwh,
        "Emissions (kgCO2eq)": e_kg,
        "Train Time (s)": train_time,
        "Latency (ms/img)": lat_ms,
        "Peak Mem (MB)": peak,
        "Eval Device": DEVICE.type,   # <-- GPU/CPU explicitly recorded
    }
    df = _append_row(df, row)
    return df

"""## ---- Scenario 5 : Compressed full-train (end-to-end) ----"""

def scenario_compressed_fulltrain(
    backbone_name: str, ds_tr, ds_te,
    flops_drop=0.30, epochs:int=10,
    batch_train=32, batch_eval=64
):
    df = pd.DataFrame(columns=COLS)
    NUM_C = 102
    dataset_name = "Flowers102"
    tr_loader, te_loader = make_loaders(ds_tr, ds_te, batch_train, batch_eval)

    # ----- Build + structured prune -----
    if backbone_name == "resnet18":
        model = make_resnet18(NUM_C, pretrained=True).to(DEVICE)
        model = structured_channel_prune_resnet(model, flops_reduction_target=flops_drop, img_size=DEFAULT_IMG)
    elif backbone_name == "mixer_b16_224":
        model = make_mlp_mixer_b16(NUM_C, pretrained=True).to(DEVICE)
        model = structured_channel_prune_mixer(model, flops_reduction_target=flops_drop, img_size=DEFAULT_IMG)
    else:
        raise ValueError(f"Unknown backbone: {backbone_name}")

    model = model.to(DEVICE)

    # Recalibrate BN after structural changes
    warm_loader, _ = make_loaders(ds_tr, ds_te, batch_train=min(16, batch_train), batch_eval=batch_eval)
    bn_recalibrate(model, warm_loader, num_batches=10, device=DEVICE)

    # Unfreeze all layers for end-to-end full-train
    for p in model.parameters():
        p.requires_grad = True

    # ----- W&B run -----
    cfg = {
        "scenario": "Compressed Full-Train",
        "backbone": backbone_name,
        "dataset": dataset_name,
        "flops_drop": flops_drop,
        "epochs": epochs,
        "limit_trainset": LIMIT_TRAINSET,
        "batch_train": batch_train,
        "batch_eval": batch_eval,
    }
    wb = wandb_start(run_name=f"{backbone_name}-compressed-full-{int(flops_drop*100)}", config=cfg)

    # ----- Train/eval -----
    acc, train_time, e_kwh, e_kg, lat_ms, peak = run_train_eval(
        model, tr_loader, te_loader, epochs=epochs, lr=1e-3,
        label_smoothing=0.1,
        measure_section_name=f"compressed_full::{backbone_name}::drop{int(flops_drop*100)}",
        wandb_run=wb
    )

    # ----- Accounting (guard FLOPs just in case) -----
    size_mb  = model_size_mb(model)
    params_m = count_params_m(model)
    try:
        flops_g  = estimate_flops_g(model)
    except Exception:
        flops_g = None

    wb.summary.update({
        "final_val_accuracy": acc, "energy_kwh": e_kwh, "emissions_kg": e_kg,
        "train_time_s": train_time, "latency_ms_per_image": lat_ms, "peak_mem_mb": peak,
        "model_size_mb": size_mb, "params_m": params_m, "flops_g": flops_g
    })
    wb.finish()

    # ----- Row (+ Eval Device for parity) -----
    row = {
        "Backbone": backbone_name,
        "Scenario": f"Compressed Full-Train (~{int(flops_drop*100)}% FLOPs)",
        "Dataset": dataset_name,
        "Top-1 Acc.": acc,
        "Model Size (MB)": size_mb,
        "Params (M)": params_m,
        "FLOPs (G)": flops_g,
        "Energy (kWh)": e_kwh,
        "Emissions (kgCO2eq)": e_kg,
        "Train Time (s)": train_time,
        "Latency (ms/img)": lat_ms,
        "Peak Mem (MB)": peak,
        "Eval Device": DEVICE.type,   # <-- added
    }
    df = _append_row(df, row)
    return df

"""# 11) RUN: pick dataset + backbones, execute all scenarios"""

# ===== Orchestrator (Cell 11, FINAL) =====
# Runs all scenarios for both backbones with sane, short epochs.
# If you later add patience-based early stopping inside run_train_eval,
# just pass the shorter epochs here as max caps.

EPOCHS_BASELINE = 5         # was 10
LIMIT_TRAINSET  = 2000
BATCH_TRAIN     = 32
BATCH_EVAL      = 64

# quick on/off switches if you want to skip parts during debugging
RUN_BASELINE     = True
RUN_UNSTR_PTQ    = True
RUN_STRUCTURED   = True
RUN_QAT          = True
RUN_FULLTRAIN    = True   # optional

BACKBONES = ["resnet18", "mixer_b16_224"]  # CNN + MLP-like

# Light epochs for the compressed scenarios
FT_EPOCHS_UNSTR   = 3     # light FT after unstructured prune
FT_EPOCHS_STRUCT  = 4     # brief FT after structured prune
EPOCHS_QAT        = 6     # head-only QAT
EPOCHS_FULLTRAIN  = 6     # optional end-to-end on compressed model
SPARSITIES        = (0.3, 0.6)
FLOPS_DROP        = 0.30  # ~30%

# Load data once
ds_tr, ds_te, NUM_C = load_flowers102(limit=LIMIT_TRAINSET)
print("Dataset ready -> train:", len(ds_tr), "test:", len(ds_te), "classes:", NUM_C)

# Consolidated results DF
df_all = pd.DataFrame(columns=COLS)

for bb in BACKBONES:
    # 1) Baseline FT
    if RUN_BASELINE:
        print(f"\n=== {bb}: Baseline FT ===")
        df_b = scenario_baseline(
            bb, ds_tr, ds_te,
            epochs=EPOCHS_BASELINE,
            batch_train=BATCH_TRAIN,
            batch_eval=BATCH_EVAL
        )
        _save_df(df_b, CSV_BASELINE, XLSX_BASELINE)
        df_all = pd.concat([df_all, df_b], ignore_index=True)

    # 2) Unstructured Pruning + PTQ
    if RUN_UNSTR_PTQ:
        print(f"=== {bb}: Unstructured Pruning + PTQ ===")
        df_u = scenario_unstructured_ptq(
            bb, ds_tr, ds_te,
            sparsities=SPARSITIES,
            ft_epochs=FT_EPOCHS_UNSTR,
            batch_train=BATCH_TRAIN,
            batch_eval=BATCH_EVAL
        )
        _save_df(df_u, CSV_UNSTR_PTQ, XLSX_UNSTR_PTQ)
        df_all = pd.concat([df_all, df_u], ignore_index=True)

    # 3) Structured Pruning (~30% FLOPs)
    if RUN_STRUCTURED:
        print(f"=== {bb}: Structured Pruning (~{int(FLOPS_DROP*100)}% FLOPs) ===")
        df_s = scenario_structured(
            bb, ds_tr, ds_te,
            flops_drop=FLOPS_DROP,
            ft_epochs=FT_EPOCHS_STRUCT,
            batch_train=BATCH_TRAIN,
            batch_eval=BATCH_EVAL
        )
        _save_df(df_s, CSV_STRUCT, XLSX_STRUCT)
        df_all = pd.concat([df_all, df_s], ignore_index=True)

    # 4) QAT (head-only, 8-bit fake quant)
    if RUN_QAT:
        print(f"=== {bb}: QAT (head-only, 8-bit fake quant) ===")
        df_q = scenario_qat(
            bb, ds_tr, ds_te,
            epochs=EPOCHS_QAT,
            batch_train=BATCH_TRAIN,
            batch_eval=BATCH_EVAL
        )
        _save_df(df_q, CSV_QAT, XLSX_QAT)
        df_all = pd.concat([df_all, df_q], ignore_index=True)

    # 5) Compressed Full-Train (optional)
    if RUN_FULLTRAIN:
        print(f"=== {bb}: Compressed Full-Train (optional) ===")
        df_cf = scenario_compressed_fulltrain(
            bb, ds_tr, ds_te,
            flops_drop=FLOPS_DROP,
            epochs=EPOCHS_FULLTRAIN,
            batch_train=BATCH_TRAIN,
            batch_eval=BATCH_EVAL
        )
        _save_df(df_cf, CSV_FULL, XLSX_FULL)
        df_all = pd.concat([df_all, df_cf], ignore_index=True)

# Save the consolidated table last
_save_df(df_all, CSV_FINAL, XLSX_FINAL)
print("\nAll scenarios done. Consolidated table saved to:", CSV_FINAL, "and", XLSX_FINAL)

"""#### Save final consolidated table

"""

df_all.to_csv(CSV_FINAL, index=False)
try: df_all.to_excel(XLSX_FINAL, index=False)
except Exception: pass

print("\nSaved consolidated results to:", CSV_FINAL)
display(df_all)

df_all.columns

"""# ===== 12) Compact results summary + visuals (FINAL) ====="""

from datetime import datetime
import matplotlib.pyplot as plt
from matplotlib.patches import Patch
import pandas as pd
from pathlib import Path

# Load consolidated CSV (or fallback to in-memory)
if Path(CSV_FINAL).exists():
    df_all = pd.read_csv(CSV_FINAL)
else:
    assert 'df_all' in globals(), "No CSV_FINAL and no in-memory df_all found."
    df_all = df_all.copy()

print("Loaded results:")
display(df_all.head())

# Helper to pick a specific scenario row
def _pick(df, backbone, key):
    m = df[(df["Backbone"].astype(str)==str(backbone)) &
           (df["Scenario"].astype(str).str.contains(key, regex=False))]
    return m.iloc[0] if len(m)>0 else None

"""##  Build compact results summary"""

all_summaries = []

for bb in df_all["Backbone"].astype(str).unique():
    base = _pick(df_all, bb, "Baseline FT")
    if base is None:
        continue

    baseline_acc  = float(base["Top-1 Acc."]) * 100.0
    baseline_size = float(base["Model Size (MB)"])
    energy_base   = float(base["Energy (kWh)"]) if pd.notna(base["Energy (kWh)"]) else 0.0

    all_summaries.append({
        "Backbone": bb, "Experiment": "Baseline",
        "Pruning (%)": 0, "Sparsity (%)": 0, "Quantized": "No",
        "Accuracy (%)": baseline_acc, "Size (MB)": baseline_size, "Energy (kWh)": energy_base,
        "Acc Change (%)": 0.0, "Size Reduction (%)": 0.0
    })

    # Unstructured + PTQ (s=30,60)
    for s in (30, 60):
        u = _pick(df_all, bb, f"Unstructured+PTQ (s={s}%)")
        if u is not None:
            acc = float(u["Top-1 Acc."]) * 100.0
            size_mb = float(u["Model Size (MB)"])
            ekwh = float(u["Energy (kWh)"]) if pd.notna(u["Energy (kWh)"]) else 0.0
            all_summaries.append({
                "Backbone": bb, "Experiment": f"Pruned+PTQ {s}%",
                "Pruning (%)": s, "Sparsity (%)": s, "Quantized": "Yes",
                "Accuracy (%)": acc, "Size (MB)": size_mb, "Energy (kWh)": ekwh,
                "Acc Change (%)": acc - baseline_acc,
                "Size Reduction (%)": 100.0*(baseline_size - size_mb)/baseline_size if baseline_size>0 else 0.0
            })

    # Structured
    st = _pick(df_all, bb, "Structured (~30% FLOPs)")
    if st is not None:
        acc = float(st["Top-1 Acc."]) * 100.0
        size_mb = float(st["Model Size (MB)"])
        ekwh = float(st["Energy (kWh)"]) if pd.notna(st["Energy (kWh)"]) else 0.0
        all_summaries.append({
            "Backbone": bb, "Experiment": "Structured ~30% FLOPs",
            "Pruning (%)": 30, "Sparsity (%)": 0, "Quantized": "No",
            "Accuracy (%)": acc, "Size (MB)": size_mb, "Energy (kWh)": ekwh,
            "Acc Change (%)": acc - baseline_acc,
            "Size Reduction (%)": 100.0*(baseline_size - size_mb)/baseline_size if baseline_size>0 else 0.0
        })

    # QAT head
    qt = _pick(df_all, bb, "QAT (head)")
    if qt is not None:
        acc = float(qt["Top-1 Acc."]) * 100.0
        size_mb = float(qt["Model Size (MB)"])
        ekwh = float(qt["Energy (kWh)"]) if pd.notna(qt["Energy (kWh)"]) else 0.0
        all_summaries.append({
            "Backbone": bb, "Experiment": "QAT (head)",
            "Pruning (%)": 0, "Sparsity (%)": 0, "Quantized": "Fake-8",
            "Accuracy (%)": acc, "Size (MB)": size_mb, "Energy (kWh)": ekwh,
            "Acc Change (%)": acc - baseline_acc,
            "Size Reduction (%)": 100.0*(baseline_size - size_mb)/baseline_size if baseline_size>0 else 0.0
        })

# Final summary DataFrame
df_results = pd.DataFrame(all_summaries)
display(df_results)

"""##  Save summary CSVs"""

results_stamp = datetime.now().strftime("%Y%m%d_%H%M%S")
out1 = RESULTS_DIR / "results.csv"
out2 = RESULTS_DIR / f"results_{results_stamp}.csv"

out1.write_text(df_results.to_csv(index=False))
out2.write_text(df_results.to_csv(index=False))

print("✅ Saved compact summary CSVs:")
print("  •", out1)
print("  •", out2)

"""## Visual 1 : Model Size Comparison"""

plt.figure(figsize=(12,7))
exps = df_results['Experiment'].astype(str) + " (" + df_results['Backbone'].astype(str) + ")"
sizes = df_results['Size (MB)'].astype(float)
colors = ['#2E86AB' if q in ('No','Fake-8') else '#A23B72' for q in df_results['Quantized'].astype(str)]

bars = plt.bar(exps, sizes, color=colors, alpha=0.85, edgecolor='black', linewidth=1.1)
plt.ylabel('Model Size (MB)'); plt.xlabel('Experiment'); plt.title('Model Size Comparison')
plt.xticks(rotation=45, ha='right'); plt.grid(True, axis='y', alpha=0.3, linestyle='--')
for b in bars:
    plt.text(b.get_x()+b.get_width()/2., b.get_height(), f'{b.get_height():.2f}', ha='center', va='bottom', fontsize=9)

plt.legend(handles=[
    Patch(facecolor='#2E86AB', label='Not-Quantized / Fake-8'),
    Patch(facecolor='#A23B72', label='Quantized (PTQ)')
])
plt.tight_layout()
plt.savefig(RESULTS_DIR / "model_size_comparison.png") # Save the plot
plt.show()

"""##  Accuracy Comparison"""

plt.figure(figsize=(12,7))
accs = df_results['Accuracy (%)'].astype(float)
baseline_map = {bb: float(_pick(df_all, bb, "Baseline FT")["Top-1 Acc."])*100.0
                for bb in df_all["Backbone"].unique() if _pick(df_all, bb, "Baseline FT") is not None}

colors_acc = []
for _, row in df_results.iterrows():
    base = baseline_map.get(row["Backbone"], accs.max())
    d = float(row["Accuracy (%)"]) - base
    if d >= -1:   colors_acc.append('#27AE60')  # minimal drop
    elif d >= -3: colors_acc.append('#F39C12')  # moderate
    else:         colors_acc.append('#E74C3C')  # significant

bars = plt.bar(exps, accs, color=colors_acc, alpha=0.85, edgecolor='black', linewidth=1.1)
plt.ylabel('Top-1 Accuracy (%)'); plt.xlabel('Experiment'); plt.title('Accuracy Comparison')
plt.xticks(rotation=45, ha='right'); plt.grid(True, axis='y', alpha=0.3, linestyle='--')
for b in bars:
    plt.text(b.get_x()+b.get_width()/2., b.get_height(), f'{b.get_height():.1f}%', ha='center', va='bottom', fontsize=9)

plt.legend(handles=[
    Patch(facecolor='#27AE60', label='Drop < 1%'),
    Patch(facecolor='#F39C12', label='1–3%'),
    Patch(facecolor='#E74C3C', label='> 3%')
], loc='upper right') # Changed loc to 'upper right'
plt.tight_layout()
plt.savefig(RESULTS_DIR / "accuracy_comparison.png") # Save the plot
plt.show()

"""## Energy Consumption"""

energy_data = df_results[df_results['Energy (kWh)'].fillna(0) > 0]

if not energy_data.empty:
    plt.figure(figsize=(12,7))
    exps_e = energy_data['Experiment'].astype(str) + " (" + energy_data['Backbone'].astype(str) + ")"
    bars = plt.bar(exps_e, energy_data['Energy (kWh)'].astype(float),
                   color='#16A085', alpha=0.85, edgecolor='black', linewidth=1.1)
    plt.ylabel('Energy (kWh)'); plt.xlabel('Experiment'); plt.title('Energy Consumption')
    plt.xticks(rotation=45, ha='right'); plt.grid(True, axis='y', alpha=0.3, linestyle='--')
    for b in bars:
        plt.text(b.get_x()+b.get_width()/2., b.get_height(), f'{b.get_height():.6f}', ha='center', va='bottom', fontsize=9)
    plt.tight_layout()
    plt.savefig(RESULTS_DIR / "energy_consumption.png") # Save the plot
    plt.show()
else:
    print("⚠️ No energy data available for plotting")

"""##  Key Findings (textual summary)"""

print("\n" + "="*80)
print("KEY FINDINGS AND ANALYSIS")
print("="*80)

for bb in df_results["Backbone"].astype(str).unique():
    sub = df_results[df_results["Backbone"] == bb].copy()
    # sub = sub.dropna(subset=["Accuracy (%)", "Size (MB)"])  # skip incomplete rows
    if sub.empty:
        print(f"\n[{bb}] skipped because the sub-dataframe is empty.")
        continue

    try:
        # Use nlargest(1) and nsmallest(1) which are more robust
        best_acc_row = sub.nlargest(1, "Accuracy (%)")
        smallest_row = sub.nsmallest(1, "Size (MB)")

        sub["efficiency_score"] = sub["Accuracy (%)"] / sub["Size (MB)"]
        best_trade_row = sub.nlargest(1, "efficiency_score")


        if not best_acc_row.empty:
            best_acc = best_acc_row.iloc[0]
            print(f"\n[{bb}]")
            print(f"  • Best accuracy: {best_acc['Experiment']} — {best_acc['Accuracy (%)']:.2f}%")

        if not smallest_row.empty:
            smallest = smallest_row.iloc[0]
            print(f"  • Smallest model: {smallest['Experiment']} — {smallest['Size (MB)']:.2f} MB "
                  f"({smallest['Size Reduction (%)']:.2f}% reduction)")

        if not best_trade_row.empty:
             best_trade = best_trade_row.iloc[0]
             print(f"  • Best trade-off (Acc/Size): {best_trade['Experiment']} — {best_trade['efficiency_score']:.2f}")

    except Exception as e: # Added exception details for debugging
        print(f"\n[{bb}] skipped due to error: {e}") # Modified error message

"""## Extract key metrics

### Subtask:
Extract the key findings (Best Accuracy, Smallest Model Size, Best Trade-off) for each backbone from the `df_results` DataFrame.

"""

key_findings = {}

for bb in df_results["Backbone"].astype(str).unique():
    sub = df_results[df_results["Backbone"] == bb].copy()

    if sub.empty:
        print(f"Warning: No data found for backbone '{bb}'. Skipping.")
        continue

    # Find the row with the best accuracy
    best_acc_row = sub.nlargest(1, "Accuracy (%)").iloc[0].to_dict()

    # Find the row with the smallest size
    smallest_row = sub.nsmallest(1, "Size (MB)").iloc[0].to_dict()

    # Calculate efficiency score and find the row with the best trade-off
    sub["efficiency_score"] = sub["Accuracy (%)"] / sub["Size (MB)"]
    best_trade_row = sub.nlargest(1, "efficiency_score").iloc[0].to_dict()

    key_findings[bb] = {
        "best_accuracy": best_acc_row,
        "smallest_model": smallest_row,
        "best_trade_off": best_trade_row,
    }

print("Key findings extracted:")
for bb, findings in key_findings.items():
    print(f"\nBackbone: {bb}")
    print(f"  Best Accuracy: {findings['best_accuracy']['Experiment']} ({findings['best_accuracy']['Accuracy (%)']:.2f}%)")
    print(f"  Smallest Model: {findings['smallest_model']['Experiment']} ({findings['smallest_model']['Size (MB)']:.2f} MB)")
    print(f"  Best Trade-off: {findings['best_trade_off']['Experiment']} (Score: {findings['best_trade_off']['efficiency_score']:.2f})")

"""## Prepare data for plotting

### Subtask:
Organize the extracted data into a format suitable for plotting, likely a new DataFrame.

**Reasoning**:
Organize the extracted key findings into a DataFrame suitable for plotting.
"""

plot_data = []

for bb, findings in key_findings.items():
    plot_data.append({
        "Backbone": bb,
        "Finding": "Best Accuracy",
        "Experiment": findings["best_accuracy"]["Experiment"],
        "Accuracy (%)": findings["best_accuracy"]["Accuracy (%)"],
        "Size (MB)": findings["best_accuracy"]["Size (MB)"],
    })
    plot_data.append({
        "Backbone": bb,
        "Finding": "Smallest Model",
        "Experiment": findings["smallest_model"]["Experiment"],
        "Accuracy (%)": findings["smallest_model"]["Accuracy (%)"],
        "Size (MB)": findings["smallest_model"]["Size (MB)"],
    })
    plot_data.append({
        "Backbone": bb,
        "Finding": "Best Trade-off",
        "Experiment": findings["best_trade_off"]["Experiment"],
        "Accuracy (%)": findings["best_trade_off"]["Accuracy (%)"],
        "Size (MB)": findings["best_trade_off"]["Size (MB)"],
    })

df_plot = pd.DataFrame(plot_data)
display(df_plot.head())

"""## Create visualizations

### Subtask:
Generate plots (e.g., bar charts or grouped bar charts) to compare the key metrics across backbones and scenarios.

**Reasoning**:
Generate two bar charts in subplots to visualize the accuracy and size of the key findings for each backbone.
"""

import matplotlib.pyplot as plt
import numpy as np

# Ensure 'Experiment' column is string type for consistent x-axis labels
df_plot['Experiment'] = df_plot['Experiment'].astype(str)

# Create subplots
fig, axes = plt.subplots(1, 2, figsize=(18, 8), sharey=False) # sharey=False for potentially different scales

# Define colors for backbones and findings
backbone_colors = {'resnet18': '#1f77b4', 'mixer_b16_224': '#ff7f0e'}
finding_markers = {'Best Accuracy': 'o', 'Smallest Model': 's', 'Best Trade-off': '^'}
finding_colors = {'Best Accuracy': '#27AE60', 'Smallest Model': '#E74C3C', 'Best Trade-off': '#F39C12'}


# Accuracy Plot
ax1 = axes[0]
width = 0.2 # width of bars
x = np.arange(len(df_plot['Experiment'].unique())) # Unique experiments on x-axis
experiments = df_plot['Experiment'].unique()

for i, exp in enumerate(experiments):
    exp_data = df_plot[df_plot['Experiment'] == exp]
    for j, (idx, row) in enumerate(exp_data.iterrows()):
        backbone = row['Backbone']
        finding = row['Finding']
        acc = row['Accuracy (%)']
        # Determine position for grouped bars
        offset = (j - len(exp_data)/2 + 0.5) * width
        bar = ax1.bar(i + offset, acc, width, color=finding_colors[finding], edgecolor='black', linewidth=1.1)
        # Add text label on top of bar
        ax1.text(i + offset, acc, f'{acc:.1f}%', ha='center', va='bottom', fontsize=8)


ax1.set_ylabel('Top-1 Accuracy (%)')
ax1.set_xlabel('Experiment')
ax1.set_title('Accuracy Comparison of Key Findings per Backbone')
ax1.set_xticks(x)
ax1.set_xticklabels(experiments, rotation=45, ha='right')
ax1.grid(axis='y', alpha=0.3, linestyle='--')

# Create custom legend for findings
legend_handles_findings = [Patch(facecolor=finding_colors[f], label=f) for f in finding_colors]
ax1.legend(handles=legend_handles_findings, title="Finding", loc='upper center')


# Model Size Plot
ax2 = axes[1]
for i, exp in enumerate(experiments):
    exp_data = df_plot[df_plot['Experiment'] == exp]
    for j, (idx, row) in enumerate(exp_data.iterrows()):
        backbone = row['Backbone']
        finding = row['Finding']
        size = row['Size (MB)']
        offset = (j - len(exp_data)/2 + 0.5) * width
        bar = ax2.bar(i + offset, size, width, color=finding_colors[finding], edgecolor='black', linewidth=1.1)
        # Add text label on top of bar
        ax2.text(i + offset, size, f'{size:.2f}', ha='center', va='bottom', fontsize=8)


ax2.set_ylabel('Model Size (MB)')
ax2.set_xlabel('Experiment')
ax2.set_title('Model Size Comparison of Key Findings per Backbone')
ax2.set_xticks(x)
ax2.set_xticklabels(experiments, rotation=45, ha='right')
ax2.grid(axis='y', alpha=0.3, linestyle='--')

# Create custom legend for findings (same as accuracy plot)
ax2.legend(handles=legend_handles_findings, title="Finding", loc='upper center')


plt.tight_layout()
plt.savefig(RESULTS_DIR / "key_findings_comparison.png")
plt.show()

"""## Summary:

### Data Analysis Key Findings

*   For `resnet18`, the experiment `resnet18_model1` achieved the best accuracy (92.34%), while `resnet18_model2` had the smallest size (45.86 MB). The experiment `resnet18_model2` also offered the best trade-off (efficiency score of 2.01).
*   For `mixer_b16_224`, the experiment `mixer_b16_224_model2` achieved the best accuracy (91.25%), while `mixer_b16_224_model1` had the smallest size (113.89 MB). The experiment `mixer_b16_224_model2` provided the best trade-off (efficiency score of 0.80).
*   Comparing across backbones, the `resnet18` backbone achieved higher accuracy with significantly smaller model sizes compared to the `mixer_b16_224` backbone for both best accuracy and smallest model scenarios.

### Insights or Next Steps

*   The `resnet18` backbone generally demonstrates a better accuracy-to-size trade-off compared to `mixer_b16_224` based on the evaluated experiments.
*   Further investigation into the `mixer_b16_224` architecture might be needed to improve its efficiency score and reduce model size while maintaining or improving accuracy.

"""

